{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c4a499-2c35-4b67-9186-db5a23e134b0",
   "metadata": {},
   "source": [
    "### Leader-Board\n",
    "##### https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3349dc1-756c-433b-8c2d-bdb1c4954fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import transformers\n",
    "import torch\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8dd06-1925-4676-9b10-c856cb95d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_tok = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dda03-4cf8-4e42-b6dc-ebc0e5e1c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587cf83d-6e8e-4244-98d4-3182e64f62af",
   "metadata": {},
   "source": [
    "### Installed Model List (docker: fingerai/llm) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cea6396-ba42-47c6-b485-f30cfe717c70",
   "metadata": {},
   "source": [
    "v1.0\n",
    "beomi/Yi-Ko-6By\n",
    "kyujinpy/Ko-PlatYi-6B-gu\n",
    "Chang-Su/llama-2-7b-chat-ko\n",
    "kfkas/Llama-2-ko-7b-Chat\n",
    "dev7halo/falcon-7b-sharded-bf16-KoAlpaca\n",
    "davidkim205/komt-mistral-7b-v1\n",
    "amphora/small-instruct\n",
    "davidkim205/komt-mistral-7b-v1\n",
    "kakaobank/kf-deberta-base\n",
    "amphora/KorFinASC-XLM-RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6beaec1a-ba6c-40b4-953c-334f86e324ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from scipy->bitsandbytes) (1.26.2)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0850cfee-0e43-4093-847a-9a385ee7b604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'access_tok' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m      6\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m      7\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m      8\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m400\u001b[39m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m     17\u001b[0m     load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m     bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m     bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-chat-hf\u001b[39m\u001b[38;5;124m'\u001b[39m, token\u001b[38;5;241m=\u001b[39m\u001b[43maccess_tok\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'access_tok' is not defined"
     ]
    }
   ],
   "source": [
    "# Case: Load model directly\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "generation_config = dict(\n",
    "    temperature=0.3,\n",
    "    top_k=40,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    num_beams=1,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=400\n",
    "    )\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=access_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830611ff-c7c3-4cf6-9796-dd5e620fe0a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    'meta-llama/Llama-2-7b-chat-hf',\n",
    "    # low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    "    token=access_tok\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained('Chang-Su/llama-2-7b-chat-ko')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, 'Chang-Su/llama-2-7b-chat-ko')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aeb69b-a07f-4015-9de3-a522d1d8e5ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_text = '신용등급 알려줘'\n",
    "with torch.no_grad():\n",
    "    print(\"Start inference.\")\n",
    "    results = []\n",
    "    inputs = tokenizer(input_text,return_tensors=\"pt\")  #add_special_tokens=False ?\n",
    "    generation_output = model.generate(\n",
    "        input_ids = inputs[\"input_ids\"].to('cuda:0'),\n",
    "        attention_mask = inputs['attention_mask'].to('cuda:0'),\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        **generation_config\n",
    "    )\n",
    "    s = generation_output[0]\n",
    "    output = tokenizer.decode(s,skip_special_tokens=True)\n",
    "\n",
    "    response = output.split(\"### Response:\")[0].strip()\n",
    "    print(f\"====================\")\n",
    "    print(f\"Input: '{input_text}'\\n\")\n",
    "    print(f\"Output: {response}\\n\")\n",
    "\n",
    "    results.append({\"Input\":input_text,\"Output\":response})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a3d791-51ed-4fbd-a44d-a124498fade9",
   "metadata": {},
   "source": [
    "#### Ko-LLM 2 \n",
    "##### https://huggingface.co/kfkas/Llama-2-ko-7b-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cac1b8-72d7-4f42-a56e-2dbfe5304416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x, model, tokenizer, device):\n",
    "    prompt = (\n",
    "        f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요.\\n\\n### 명령어:\\n{x}\\n\\n### 응답:\"\n",
    "    )\n",
    "    len_prompt = len(prompt)\n",
    "    gened = model.generate(\n",
    "        **tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\n",
    "            device\n",
    "        ),\n",
    "        max_new_tokens=1024,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        top_k=20,\n",
    "        top_p=0.92,\n",
    "        no_repeat_ngram_size=3,\n",
    "        eos_token_id=2,\n",
    "        repetition_penalty=1.2,\n",
    "        num_beams=3\n",
    "    )\n",
    "    return tokenizer.decode(gened[0])[len_prompt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286ce00a-015e-44dd-ae7a-c2c3fae79cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_infer(input):\n",
    "    device = (\n",
    "        torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    )\n",
    "    model_id = \"kfkas/Llama-2-ko-7b-Chat\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, device_map={\"\": 0},torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model.eval()\n",
    "    model.config.use_cache = (True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    output = gen(input, model=model, tokenizer=tokenizer, device=device)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9e778-171a-495c-9b6c-a3d51361e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = LLM_infer(\"너는 누구야 ? \")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1b79d-d1a0-40d0-a8ce-cb8cf6501b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = LLM_infer(\"신용등급 알려줘\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bc1285-ce08-4fa6-8a3a-d1ede4ef70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = LLM_infer(\"DNA는 무엇의 약자인가요\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d03db4-0b55-4b79-9c1c-b31df5a52692",
   "metadata": {},
   "source": [
    "#### Ko-LLM 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba960b3-e8b7-4a58-9c1d-b75c4561c790",
   "metadata": {},
   "outputs": [],
   "source": [
    "config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af1c11-e820-4779-beb1-957185734e31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import FalconModel, FalconConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('ybelkada/falcon-7b-sharded-bf16', token=access_tok)\n",
    "peft_model_id = 'dev7halo/falcon-7b-sharded-bf16-KoAlpaca'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config._name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config._name_or_path)\n",
    "\n",
    "model = model.to('cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f85c3e3-5997-44c3-9977-ced3211b95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"광해군은 폭군이었나요 ?\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=756)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f2cb0-ad78-4038-bbe3-b6e3eb1075d6",
   "metadata": {},
   "source": [
    "#### Ko-LLM 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4423b6f-fc07-450e-887b-5365c87e6e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"davidkim205/komt-mistral-7b-v1\"\n",
    "config = AutoConfig.from_pretrained('davidkim205/komt-mistral-7b-v1', token=access_tok)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0}, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679ed328-c267-45ef-b50a-1c9831b132d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import TextStreamer, GenerationConfig\n",
    "\n",
    "model_name='davidkim205/komt-mistral-7b-v1'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name) # , device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa74fbf-8c0b-4fdd-9c15-6ab74828f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(g_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d4e0c-151e-4bfd-b1ca-3988af5297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ba898-22ae-484f-aaf1-6a9d181cf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(x):\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.8,\n",
    "        top_p=0.8,\n",
    "        top_k=100,\n",
    "        max_new_tokens=1024,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    q = f\"[INST]{x} [/INST]\"\n",
    "    gened = model.generate(\n",
    "        **tokenizer(\n",
    "            q,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=False\n",
    "        ).to('cuda'),\n",
    "        generation_config=generation_config,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    result_str = tokenizer.decode(gened[0])\n",
    "\n",
    "    start_tag = f\"\\n\\n### Response: \"\n",
    "    start_index = result_str.find(start_tag)\n",
    "\n",
    "    if start_index != -1:\n",
    "        result_str = result_str[start_index + len(start_tag):].strip()\n",
    "    return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca2419-dee9-433e-8429-3657922f5e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "txt = '금리가 물가에 미치는 영향을 설명해주세요'\n",
    "gen(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd36defd-9f53-4bf0-b0ef-6d44ed6bfd8d",
   "metadata": {},
   "source": [
    "#### Ko-LLM 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9979585-bebf-44fc-a52a-91326c173455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"kakaobank/kf-deberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"kakaobank/kf-deberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcef5430-cf35-426d-83b1-c6ac793ed2aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e242f-469a-4daf-9795-d91ce271f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab  #  핑개 -> UNKONW "
   ]
  },
  {
   "cell_type": "raw",
   "id": "158fd6fb-adea-4c51-a24b-465be6bc6e1b",
   "metadata": {},
   "source": [
    "tokenizer.encode('"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ee48f-df60-46dc-ad08-ee4647e5c4a6",
   "metadata": {},
   "source": [
    "#### Ko-LLM 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d77c9f-a7c5-4a0e-98e2-3584b4a9fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"amphora/olaf-v.42.0.2\"\n",
    "config = AutoConfig.from_pretrained(model_id, token=access_tok)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map={\"\": 0}, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be521ed-8fb9-4674-81c0-acb0e89e2f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0630a-57e1-4487-84e8-cf38a5d25586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForCausalLM, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "input_str = \"장 전체가 폭락한 가운데 삼성전자만 상승세를 이어갔다. </s> 삼성전자\"\n",
    "input = tokenizer(input_str, return_tensors='pt')\n",
    "output =model.generate(**input, max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d6758-68dd-4a7c-882a-9f873c6ea326",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d909f0-2ec7-4a7e-9afe-a38fba39e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24718bf8-693c-4b29-9a62-01d600ea4d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566a89c-2cfe-436a-9a2c-9d8e41773811",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e150bcd8-8cc3-4df9-9247-5061a84764b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabs = {d: k for k, d in tokenizer.vocab.items()}\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03708345-ce7b-4f6a-b62b-6fc17926ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs[135644]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fbb27a-89c9-4a26-adfa-6d223cbbfb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaForCausalLM, AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"amphora/olaf-v.42.0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"amphora/olaf-v.42.0.2\")\n",
    "\n",
    "input_str = \"금리가 물가에 미치는 영향을 설명해주세요\"\n",
    "input = tokenizer(input_str, return_tensors='pt')\n",
    "output =model.generate(**input, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc335d-ec88-4695-84b1-13f74fda87a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10af9de-7d59-4eea-9236-8041836fd330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
