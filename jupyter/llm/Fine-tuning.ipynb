{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360124d4-d882-4338-a45a-d1ff2887f619",
   "metadata": {},
   "source": [
    "### Defalut Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44bc3365-8102-4433-a0f8-63e3b25d8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e08771a-c0bb-42e1-8508-9348c3a134a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = os.getcwd()\n",
    "data_path = os.path.join(default_path, '../../data')\n",
    "model_path = os.path.join(default_path, '../../models')\n",
    "local_mistral_dir = os.path.join(model_path, \"mistral_origin\")\n",
    "config_path = os.path.join(default_path, '../../config')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54aa45f1-ffda-4d92-ae87-cdbcf969bae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "632546be-5413-4774-bd50-f77ba2a1462f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f59c2744-8f18-4ba3-9d81-10ffce94bc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5140\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"kaitchup/ultrachat-100k-flattened\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47c3ff5e-60a7-4d31-84e2-baefbd7aa20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "684d09f8-0729-4a3c-aa14-0fa465fd58ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Human: Create a salsa song with lyrics that capture the intensity and fervor of a passionate love affair between two people. The song should have a fast-paced tempo and incorporate traditional salsa instrumentation such as brass, percussion, and piano. The lyrics should paint a vivid picture of the emotions involved in a passionate romance, including love, desire, yearning, and heartbreak. Feel free to draw inspiration from real-life experiences or fictional narratives. At the end of the song, listeners should be left feeling captivated and moved by the powerful message of the song.### Assistant: Verse 1:\\nDesde el momento en que te vi\\nYo supe que ibas a ser para mí\\nTu cariño me envuelve \\nCon tu amor siempre quiero seguir\\n\\nChorus:\\nEl fuego arde en mi pecho\\nCuando estoy contigo, mi amor\\nEste romance es intenso\\nY nunca se acabará, jamás\\n\\nVerse 2:\\nTu sonrisa me encanta\\nY tus ojos me hacen sentir \\nQue eres mi otra mitad\\nY no puedo vivir sin ti \\n\\nChorus:\\nEl fuego arde en mi pecho\\nCuando estoy contigo, mi amor\\nEste romance es intenso\\nY nunca se acabará, jamás\\n\\nBridge:\\nPero a pesar de todo\\nSiento que algo nos separa\\nDime, corazón, ¿qué pasa?\\n¿Será que me estás dejando?\\n\\nChorus:\\nEl fuego arde en mi pecho\\nCuando estoy contigo, mi amor\\nEste romance es intenso\\nY nunca se acabará, jamás\\n\\nOutro:\\nAunque el camino es difícil\\nNo voy a perder mi fe\\nEste fuego que siento por ti\\nSiempre vivirá en mí.### Human: That's a great start! Can you add more descriptive words to capture the intensity and passion of the romance? Also, can you incorporate some metaphors and imagery to really make the lyrics pop?### Assistant: Verse 1:\\nDesde el momento en que te vi\\ntu mirada me atrapó, no pude resistir\\nen tus brazos, sentí la felicidad\\nde un amor que desde siempre quise encontrar\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nVerse 2:\\nTus labios me enloquecen\\ncada beso me hace temblar\\nno me importa el tiempo, ni la distancia\\nporque mi corazón siempre te va a amar\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nBridge:\\nPero a pesar de todo\\nsiento que algo nos separa\\nla llama de nuestro amor se apaga\\ny mi corazón se desgarra\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nOutro:\\nNuestro amor es como una flor\\nque crece fuerte cada día más\\ny aunque a veces hay tormentas\\nnuestro amor nunca se apagará.### Human: I love the way you used metaphors and imagery in the lyrics. Can you add more vivid descriptions of the physical sensations that come with being in a passionate love affair? For example, what does it feel like to be held in the arms of someone you love?### Assistant: Verse 1:\\nCuando tus brazos me envuelven\\nsiento mi cuerpo vibrar\\ntu amor es como una llama ardiente\\nque me llena de felicidad\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nVerse 2:\\nTu piel es como un cálido abrazo\\nque me hace sentir en casa\\ntus besos son como una explosión\\nque me deja sin aliento, sin voz\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nBridge:\\nPero a pesar de todo\\nsiento que algo nos separa\\nla llama de nuestro amor se apaga\\ny mi corazón se desgarra\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nOutro:\\nNuestro amor es un océano\\nque nunca dejará de crecer\\njuntos podemos enfrentar el mundo\\nporque nuestro amor nunca dejará de arder.### Human: This is fantastic so far! Can you add more descriptive language around the physical sensations of desire and yearning that come with being separated from someone you love? Let's really emphasize the intensity of the passion between these two people.### Assistant: Verse 1:\\nCuando tus brazos me envuelven\\nsiento mi cuerpo vibrar\\ny en mi corazón, un gran fuego\\nque nunca dejará de brillar\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nVerse 2:\\nPero cuando estás lejos, mi amor\\nsiento que muero un poco más\\nmi cuerpo grita tu nombre\\ny mi alma llora tu ausencia, sin paz\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nBridge:\\nPero a pesar de todo\\nnuestro amor es más fuerte\\nla distancia no nos puede separar\\nporque nuestra pasión siempre va a arder\\n\\nChorus:\\nEl fuego de tu amor me quema\\ncada noche sueño con tenerte\\nesta pasión ardiente nos une\\nnuestro amor es un fuego eterno\\n\\nOutro:\\nNuestro amor es como un volcán\\nque nunca deja de rugir\\nnuestra pasión es ardiente\\ny nunca va a dejar de existir.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0256807-b22e-47f4-8586-5fae2ca8c66f",
   "metadata": {},
   "source": [
    "### Load Tokenizer & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e785ea2-869b-4014-a52c-00a55c1c6216",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(local_mistral_dir, 'tokenizer'), use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a66633f4-4ebe-47cd-b3a7-69a86fb1f88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '<unk>')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "316d2b9f-6e65-423c-aa3a-2f7f21908b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.padding_side = 'right'   # right, left"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed597063-e453-4cfe-9e04-25cd3f18d8ec",
   "metadata": {},
   "source": [
    "# tokenizer = LlamaTokenizerFast.from_pretrained(os.path.join(model_dir, 'tokenizer'))   # LlamaTokenizer (x)  -> LlamaTokenizerFast (o)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16, low_cpu_mem_usage=True) # , device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a586a-5fd2-4983-8a1a-2456f94deed7",
   "metadata": {},
   "source": [
    "#### Quantize model with BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e82774a-e5d6-43c6-9e6d-84d093708437",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"bfloat16\")   # float16 -> bfloat16 : training more stable \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6286be6-4f68-434c-b10c-e95c48be5626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file /rag/jupyter/llm/../../models/mistral_origin/config.json\n",
      "Model config MistralConfig {\n",
      "  \"_name_or_path\": \"/rag/jupyter/llm/../../models/mistral_origin\",\n",
      "  \"architectures\": [\n",
      "    \"MistralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file /rag/jupyter/llm/../../models/mistral_origin/model.safetensors.index.json\n",
      "Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Detected 4-bit loading: activating 4-bit loading for this model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e10466c8dfe04a2ab90095ee30754f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing MistralForCausalLM.\n",
      "\n",
      "All the weights of MistralForCausalLM were initialized from the model checkpoint at /rag/jupyter/llm/../../models/mistral_origin.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
      "loading configuration file /rag/jupyter/llm/../../models/mistral_origin/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the model and quantize it on the fly\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "      local_mistral_dir, quantization_config=bnb_config, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "11fa380a-5b87-4a48-9077-0a6c0273b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast some modules of the model to fp32 \n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e80ce384-7cfb-4bb9-8f56-ad5efac23019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the pad token in the model\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False   # Gradient checkpointing is used by default but not compatible with caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5b3e8-b7ad-44d3-ab46-75186e6d6789",
   "metadata": {},
   "source": [
    "### QLoRA Setting (PEFT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e07cd2d-d369-4098-9b56-978982b0641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules= ['k_proj', 'q_proj', 'v_proj', 'o_proj', \"gate_proj\", \"down_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abef777c-8004-48d9-bf27-bdbc39cbf311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "        output_dir=os.path.join(model_path, 'mistral-QLoRA'),\n",
    "        #evaluation_strategy=\"steps\",\n",
    "        #do_eval=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        log_level=\"debug\",\n",
    "        save_steps=50,\n",
    "        logging_steps=10,\n",
    "        learning_rate=4e-4,\n",
    "        #eval_steps=200,\n",
    "        num_train_epochs=100,\n",
    "        max_steps=100,\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"linear\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca126bcf-53c7-42d9-96ee-c11501cb0147",
   "metadata": {},
   "source": [
    "### QLoRA Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0473503a-88e9-45dc-b5ac-ff90f5ad62cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4c74af38ce493689e012c519e6a7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset['train'],\n",
    "        #eval_dataset=dataset['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=512,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bbe29236-c1f1-4e53-8447-200f5580134f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 100,000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 41,943,040\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:19, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.395200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.204600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /rag/jupyter/llm/../../models/checkpoint-20 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to /rag/jupyter/llm/../../models/checkpoint-20\n",
      "tokenizer config file saved in /rag/jupyter/llm/../../models/checkpoint-20/tokenizer_config.json\n",
      "Special tokens file saved in /rag/jupyter/llm/../../models/checkpoint-20/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /rag/jupyter/llm/../../models/checkpoint-40 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to /rag/jupyter/llm/../../models/checkpoint-40\n",
      "tokenizer config file saved in /rag/jupyter/llm/../../models/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in /rag/jupyter/llm/../../models/checkpoint-40/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /rag/jupyter/llm/../../models/checkpoint-60 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to /rag/jupyter/llm/../../models/checkpoint-60\n",
      "tokenizer config file saved in /rag/jupyter/llm/../../models/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in /rag/jupyter/llm/../../models/checkpoint-60/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /rag/jupyter/llm/../../models/checkpoint-80 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to /rag/jupyter/llm/../../models/checkpoint-80\n",
      "tokenizer config file saved in /rag/jupyter/llm/../../models/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in /rag/jupyter/llm/../../models/checkpoint-80/special_tokens_map.json\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Checkpoint destination directory /rag/jupyter/llm/../../models/checkpoint-100 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Saving model checkpoint to /rag/jupyter/llm/../../models/checkpoint-100\n",
      "tokenizer config file saved in /rag/jupyter/llm/../../models/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /rag/jupyter/llm/../../models/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.2126054763793945, metrics={'train_runtime': 1051.0047, 'train_samples_per_second': 1.522, 'train_steps_per_second': 0.095, 'total_flos': 3.51564749340672e+16, 'train_loss': 1.2126054763793945, 'epoch': 0.02})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b18a9c-a710-4fb0-81db-f207f423108b",
   "metadata": {},
   "source": [
    "### Load QLoRA tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b3b6c68-22bc-4649-a997-246428af4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(local_mistral_dir, 'tokenizer'), use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c91404d-27ee-4252-9b13-b7e446a506ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<unk>', '</s>')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.unk_token, tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6932f598-fe68-4c9b-8250-646295cf05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.padding_side = 'right'   # right, left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc3002e6-d5a6-4bc7-a9e1-2ec83864a6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/rag/jupyter/llm/../../models/mistral_origin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_mistral_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33351d14-57eb-48c9-8a52-3fde0cba4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"bfloat16\")   # float16 -> bfloat16 : training more stable \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8df7171-c719-4222-b631-a40a23f66ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ddc709b0d849ddb6153b6342235b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and quantize it on the fly\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "      local_mistral_dir, quantization_config=bnb_config, torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51ad3dc3-38db-462f-89bd-362c9b9b77d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = True\n",
    "model = PeftModel.from_pretrained(model, \"../../models/checkpoint-100/\")   # top layer에 fine-tuned adapter 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e5c2d94-16f6-4704-a2f3-084497d93eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저는 인공지능 모델이기 때문에 개인의 상황에 따라 추천할 수 있는 금융 상품이 다를 수 있습니다. 하지만 일반적으로 추천할 수 있는 몇 가지 금융 상품은 다음과 같습니다:\n",
      "\n",
      "1. 저금리 대출: 저금리 대출은 대출 금리를 낮게 주는 대출 상품입니다. 이는 대출 금리를 낮게 주는 대출 상품으로 대출 금리를 낮게 주는 대출 상품입니다.\n",
      "\n",
      "2. 저금리 예금: 저금리 예금은 예금 금리를 낮게 주는 예금 상품입니다. 이는 예금 금리를\n"
     ]
    }
   ],
   "source": [
    "def generate(instruction):\n",
    "    prompt = \"### Human: \"+instruction+\"### Assistant: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].cuda()\n",
    "    generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=GenerationConfig(pad_token_id=tokenizer.pad_token_id, temperature=1.0, top_p=1.0, top_k=50, num_beams=1),\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=256\n",
    "    )\n",
    "    for seq in generation_output.sequences:\n",
    "        output = tokenizer.decode(seq)\n",
    "        print(output.split(\"### Assistant: \")[1].strip())\n",
    "        \n",
    "generate(\"금융 상품 추천해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc890cb-8c72-43a7-929a-75efc5e131a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6bde1bd-9df3-4a38-a505-9f4e166160cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8a5bfbf-5bfb-4ed7-a5c4-a83836b39b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2798, -1.3275, -2.5542,  0.5418],\n",
       "          [-0.3713,  0.2289,  0.0246, -1.0159],\n",
       "          [ 0.2753,  0.9804,  0.7752,  0.1774]],\n",
       "\n",
       "         [[ 0.5081, -2.3607,  0.5605, -0.4808],\n",
       "          [-1.9072, -0.8845, -0.3196,  0.1949],\n",
       "          [ 0.1367,  0.0303, -0.3085, -0.3469]]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d865c4-303b-4bde-8ce3-ba9bc64d05de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
